## Pipeline

### `AsyncEngine` API

The `AsyncEngine` class provides an asynchronous inference engine, which maintains multiple instances of a specified model for efficient processing.

#### Initialization parameters:

- **model_path** (str): Path to the model. This can be a path to a local directory storing a Turbomind model, or a model_id for models hosted on (huggingface.co)\[https://huggingface.co\].
- **model_name** (Optional\[str\]): Name of the model when the model_path points to a Pytorch model on huggingface.co. Default is None.
- **backend** (Literal\['turbomind', 'pytorch'\]): Specifies the backend to use, either turbomind or pytorch. Default is set to turbomind.
- **backend_config** (Optional\[Union\[TurbomindEngineConfig, PytorchEngineConfig\]\]): Configuration object for the backend. It can be either TurbomindEngineConfig or PytorchEngineConfig depending on the backend chosen. Default is None.
- **chat_template_config** (Optional\[ChatTemplateConfig\]): Configuration for chat template. Default is None.
- **instance_num** (int): The number of instances to be created for handling concurrent requests. Default is 32.
- **tp** (int): Number of tensor parallel units. Default is 1.

#### Methods:

- **call**(prompts, gen_config, chat_template_config, request_output_len, top_k, top_p, temperature, repetition_penalty, ignore_eos, kwargs): Inference on a batch of prompts.

- **stop_session**(session_id): Stop a session with a given id.

- **end_session**(session_id): Clear a session with a given id.

- **get_generator**(stop, session_id): Get generator for a given session.

- **batch_infer**(prompts, gen_config, chat_template_config, request_output_len, top_k, top_p, temperature, repetition_penalty, ignore_eos, kwargs): Inference on a batch of prompts.

- **generate**(messages, session_id, gen_config, chat_template_config, stream_response, sequence_start, sequence_end, step, request_output_len, stop, stop_words, top_k, top_p, temperature, repetition_penalty, ignore_eos, kwargs): Generate responses.

### `pipeline` API

The `pipeline` function is a higher-level API designed for users to easily instantiate and use the AsyncEngine.

#### Parameters:

- **model_path** (str): Path to the model. This can be a path to a local directory storing a Turbomind model, or a model_id for models hosted on huggingface.co.

- **model_name** (Optional\[str\]): Name of the model when the model_path points to a Pytorch model on huggingface.co. Default is None.

- **backend** (Literal\['turbomind', 'pytorch'\]): Specifies the backend to use, either turbomind or pytorch. Default is set to turbomind.

- **backend_config** (Optional\[Union\[TurbomindEngineConfig, PytorchEngineConfig\]\]): Configuration object for the backend. It can be either TurbomindEngineConfig or PytorchEngineConfig depending on the backend chosen. Default is None.

- **chat_template_config** (Optional\[ChatTemplateConfig\]): Configuration for chat template. Default is None.

- **instance_num** (int): The number of instances to be created for handling concurrent requests. Default is 32.

- **tp** (int): Number of tensor parallel units. Default is 1.

- **log_level** (str): The level of logging. Default is 'ERROR'.

### Example

```python
import lmdeploy
from lmdeploy.messages import GenerationConfig
from lmdeploy.pytorch import EngineConfig

# there are more arguments in EngineConfig set by default
backend_config = EngineConfig(tp = 1, session_len= 1024)

# there are more arguments in GenerationConfig set by default
gen_config = GenerationConfig(max_new_tokens=224)

# Initialize pipeline
pipe = lmdeploy.pipeline('InternLM/internlm-chat-7b-v1_1',  backend='pytorch', backend_config = backend_config)

# Perform inference on multiple inputs
response = pipe(['hi','say this is a test'], gen_config=gen_config)

print(response)

```

In the above example, pipeline function initializes an AsyncEngine with the model specified by the 'model_path'. It then performs inference on the list of prompts using the engine's `__call__` method and prints out the responses generated by the model.
